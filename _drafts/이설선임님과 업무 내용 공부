---
layout: post
categories: articles
title:  "title"
excerpt: "excerpt"
tags: []
date: 2015-10-02 23:41:42
modified: 2015-10-02 23:41:42
image: 
  feature: filename
  credit: image owner
  creditlink: original link
share: true
sitemap: true
---

1. mysql
 - 그냥 yum이나 rpm 등의 패키지 마스터로 설치해서 실행하는 mysql은 root 권한으로 실행하므로 한 서버에서 여러 유저가 독립적인 DB를 운영하기 불편한 구조
 - 만약 그냥 mysql을 사용하면 내 DB 파일이 상대방에 의해 변경되거나 나도 실수를 할 수 있음
 - 따라서 별도로 mysql을 설치해서 독립적인 id로 실행하는 것이 안전
 - 방법
  i) sss repo (10.24.142.197)에 있는 mysql 패키지를 rxync로 받아서 푼다. (::R/naver/pkgs/mysql)
   => mysql community edition(무료 오픈 버젼)을 직접 다운 받고 싶지만 모두 yum repo, 혹은 rpm 패키징 되어있어 맘에 드는 곳에 맘대로 설치하는 방법을 모르겠다.
  ii) $LD_LIBRARY_PATH를 등록한다
   => ~/.bashrc에 등록하거나 셸 스크립트 안에 집어넣는다.
   => export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$MYSQL_HOME/lib/mysql
  iii) mysql을 실행하기 위해서는 mysql db를 우선 생성해야 한다
   => $MYSQL_HOME/bin/mysql_install_db --basedir=$MYSQL_HOME --datadir=$PROJECT_HOME/mysql_data/data
  iv) my.cnf로 별도의 설정 파일도 준비한다. 네이버 패키지의 경우는 bin 폴더의 mysqld_safe 스크립트 안에 경로가 하드 코딩 되어있는 경우도 있으므로 확인하여 바꿔줘야 한다.
   => my.cnf, db data, mysql.sock 등 관련 설정 파일은 $MYSQL_PATH 하위에 두지 않는 것이 좋다. 다른 이용자에 의해 변경될 가능성이 있기 때문이다. 작업하는 프로젝트에 별도 공간을 마련하는 것이 좋다. (ex: $PROJECT_HOME/mysql_data/etc/my.cnf)
   => my.cnf 에 들어가는 정보
        [mysqld]
        datadir=$PROJECT_HOME/mysql_data/data
        socket=$PROJECT_HOME/mysql_data/var/socket/mysql.sock
        user=mysql
        symbolic-links=1
        server-id=1
        log-bin=mysql-bin
        default-storage-engine=innodb
        character-set-server=utf8
        collation-server=utf8_bin
        port=$PORT

        [mysqld_safe]
        log-error=$PROJECT_HOME/var/log/mysqld.log
        pid-file=$PROJECT_HOME/var/pid/mysqld.pid
  v) 접근 제한 오류가 날 경우 디렉토리 권한도 살펴야 한다
  vi) mysql(서버)을 백그라운드로 실행한다
   => mysqld_safe를 이용한 방법: $MYSQL_HOME/bin/mysqld_safe --defaults-file=$PROJECT_HOME/mysql_data/etc/my.cnf --datadir=$PROJECT_HOME/mysql_data/data --socket=$PROJECT_HOME/mysql_data/var/socket/mysql.sock --port=$PORT --log-error=$PROJECT_HOME/mysql_data/var/log/mysqld.log --pid-file=$PROJECT_HOME/mysql_data/var/pid/mysqld.pid &
   => mysqld_safe 관련 정보: http://linuxism.tistory.com/239
   => mysqld_safe 관련, mysql 시작과 종료: http://database.sarang.net/?inc=read&aid=23265&criteria=mysql&subcrit=tutorials&id=&limit=20&keyword=&page=2
   => init.d 의 데몬 관리를 받는 방법: /etc/init.d/mysqld start -> 일반적인 방법이지만 독립적인 mysql 서버의 실행에는 응용할 수 없다.
  vii) 클라이언트로 접속한다
   => $MYSQL_HOME/bin/mysql -S $PROJECT_HOME/mysql_data/var/socket/mysql.sock -P $PORT -h localhost -D database
 - http://www.koreapoly.co.kr/view.php?q=%EB%A6%AC%EB%88%85%EC%8A%A4mysql%EC%84%A4%EC%B9%98
 - http://wnstjqdl.tistory.com/18
 - centOS 6.5 yum 의 mysql 버젼은 5.1 이므로 업그레이드가 필요하다면 다음의 절차를 따라라.
  => irteamsu 계정으로 로그인한다.
  => http://dev.mysql.com/downloads/repo/yum/ 에서 적절한 yum repo 패키지를 wget 다운로드한다. centOS 6.5, linux 2.6 이라면 Red Hat Enterprise Linux 6 / Oracle Linux 6로 할 수 있다.
  => sudo yum localinstall mysql-community-release-el6-5.noarch.rpm
  => yum install mysql-server -> 이것으로 디펜던시까지 모두 설치 완료된다.
  => 설명: http://fsteam.tistory.com/94
 - 계정 만들기: grant로 한방에 만들 수도 있고 create user로 만든 후 grant 권한을 줄 수도 있는데 mysql은 후자를 추천
  => root 계정이나 계정 관리 권한을 가진 계정으로 mysql 접속
  => create user 'newuser'@'localhost' identified by 'password';
  => grant all (privileges) on dbname.tablename to 'newuser'@'localhost' (identified by 'password') with grant option;
  => alter, create, drop, select, insert, update, delete == 일반 사용자 권한
  => reload, shutdown == 관리자 권한
  => all == 그냥 총 권한
  => usage == no privileges 와 동의어
  => 계정 생성 혹은 권한 변경 직후 권한 적용: flush privileges;
  => http://blog.naver.com/imju1196/20164852786
  => http://blog.naver.com/marundubu/120166942800
 - 계정 삭제: drop user 'user'@'localhost'
 - mysql 테이블 복제(dump, migration)
  i) 복사(백업): mysqldump -u'root' -p -h'localhost' -S /socket_경로/mysql.sock (--default-character-set=utf8) DB명 테이블명( 테이블명) > 파일.sql
  => mysqldump: mysql bin folder안에 있는 실행파일
  => user id. '-u'와 user id를 붙여 써야 한다. (예: -umyid)
  => -p : 비밀번호를 지정. (예: -p1234)
  => -h : mysql이 실행중인 컴퓨터의 address. local인 경우 이 옵션을 사용하지 않는다. (예: -h192.168.5.46)
  => -t : 데이터만 덤프 받는다.
  => -d : schema만 덤프 받는다.
  ii) 붙이기: mysql -S /소켓 경로/mysql.sock (--default-character-set=utf8) db명 < 파일.sql
  => 서로 다른 이름의 DB 간 복사 가능
  => root 계정은 그냥 되니까 편리
  => 사용자 계정으로 할 경우 꼭 필요한 grant 권한: SELECT, SHOW VIEW, RELOAD, REPLICATION CLIENT(, EVENT, TRIGGER)
  => event, trigger는 없어도 되는 거 같고 넣으면 오류를 일으킨다. 정확한 것은 더 조사 필요
  => reload, replication client는 global 전용 권한이기 때문에 특정 데이터베이스만을 설정해서 할 수는 없다.
  => http://www.fromdual.com/privileges-of-mysql-backup-user-for-mysqldump
  => https://dev.mysql.com/doc/refman/5.1/en/grant.html
 - 데이터만 파일로 뽑기
  => into outfile '파일경로'
  => ex: select * from table into outfile '/tmp/test.txt';
  => 다양한 파라미터: into outfile 'file' fields terminated by ',' enclosed by '"' lines terminated by '\n'
  => 주의: 유저에게 write 권한이 있어야 한다. grant를 잘 체크할 것
  => http://www.tech-recipes.com/rx/1475/save-mysql-query-results-into-a-text-or-csv-file/
 - DB 안에서 쿼리로 database 사이즈 보기
  => SELECT table_schema "Data Base Name", sum( data_length + index_length ) / 1024 / 1024 "Data Base Size in MB" FROM information_schema.TABLES GROUP BY table_schema;
  => http://forums.mysql.com/read.php?108,201578,201578
 - 특정 사용자 계정 권한 보기: show grants for username;
 - 현재 기본 설정 보기: status; -> 언어 설정도 일부 보여준다
 - 현재 언어 설정 보기: show variables like "char%", show variables like "collation%" -> 한 줄에 되는 방법은 "c%"
 - UTF8 맞추기
  => server, client 모두 utf8로 맞아야 한다.
  => client의 경우: SET character_set_client = charset_name; SET character_set_results = charset_name; SET character_set_connection = charset_name;
  => 이걸 한 줄에 하는 것: set names utf8;
  => auto-reconnect가 enabled 되어있을 때 더욱 추천되는 명령어: charset utf8 (https://dev.mysql.com/doc/refman/5.0/en/charset-connection.html)
     그러나 이걸 my.cnf 에 넣으면 mysql client가 제대로 작동 안 하고 원인은 모름
 - charset vs. collation
  => character set: 심볼(글자)과 인코딩의 묶음
  => collation: 문자셋의 문자들을 비교하는 규칙 -> 문자 정렬 규칙
    ex: utf8_bin -> 바이너리 코드 순 생 정렬 (A, B, a, b) -> 별로 같아 보이지만 primary key를 포함해 대소문자 비교가 보장된다는 장점
      --> db table 만들 때 primary key 되는 놈이 varchar라면 varchar(size) binary 로 만들자.
      --> 이 때 뒤의 binary는 단독 자료형 binary와는 다른 표현으로, 해당 varchar의 charset을 유지하고 collation을 bin 종류로 만든다는 의미
      --> 이것으로 대소문자 구별을 보장할 수 있고, 이는 나중에 대소구별 없는 경우 키값이 똑같아지는 문제를 방지할 수 있다.
      --> 이를 해두지 않으면 주로 마이그레이션, 또는 alter table 할 때 문제가 생긴다.
      --> https://dev.mysql.com/doc/refman/5.7/en/binary-varbinary.html
    ex: utf8_general_ci -> 널리 쓰이는 문자 정렬 (A, a, B, b)
    ex: utf8_unicode_ci -> 다국어, 특수기호에서 좀 더 사람다운 정렬
  => http://kwonnam.pe.kr/wiki/database/mysql/charset
  => https://dev.mysql.com/doc/refman/5.0/en/charset-connection.html
 - 인코딩 변경 관련: http://ra2kstar.tistory.com/97
 - utf8mb4
  => 알려진 바로는 오직 mysql, mariadb 관련 이슈
  => mysql 버젼 5.5 이후 추가된 기능
  => 내용: 원래 유니코드는 4바이트 가변길이 설계 인코딩이다. mysql은 퍼포먼스 튜닝으로 이를 기존 3바이트 가변길이로 제한했음.
    이제 맥의 emoji, 특수기호 등이 많이 추가되면서 4바이트 유니코드가 등장하기 시작. 이를 처리하기 위해 2010년 mysql에서 업데이트 된 내용이 4바이트 utf8!
  => 5.5 이하 권장 세팅: charset=utf8, collation=utf8_unicode_ci
  => 5.5 이상 권장 세팅: charset=utf8mb4, collation=utf8mb4_unicode_c
  => utf8 -> utf8mb4 로 바꿔도 기존 데이터가 깨지지 않으므로 바꿔주자
  => 이걸 해주지 않으면 4바이트 데이터가 들어올 때 워닝이 뜨고 작업이 중단되어 이후 문자열 삽입이 안됨 (python 테스트로 확인)
  => 파이썬이나 클라이언트에서 db 접근할 때 charset도 utf8 -> utf8mb4로 재설정하는 것을 잊지 말자
  => mysql 설정 결과
    +--------------------------+----------------------------+
    | Variable_name            | Value                      |
    +--------------------------+----------------------------+
    | character_set_client     | utf8mb4                    |
    | character_set_connection | utf8mb4                    |
    | character_set_database   | utf8mb4                    |
    | character_set_filesystem | binary                     |
    | character_set_results    | utf8mb4                    |
    | character_set_server     | utf8mb4                    |
    | character_set_system     | utf8                       |
    | collation_connection     | utf8mb4_unicode_ci         |
    | collation_database       | utf8mb4_unicode_ci         |
    | collation_server         | utf8mb4_unicode_ci         |
    +--------------------------+----------------------------+
  => my.cnf 설정 결과
    [client]
    default-character-set=utf8mb4

    [mysql]
    default-character-set=utf8mb4

    [mysqld]
    datadir=/naver/appGuard/goo/glePlayParser/mysql_data/data
    socket=/naver/appGuard/googlePlayParser/mysql_data/var/socket/mysql.sock
    user=irteam
    # Disabling symbolic-links is recommended to prevent assorted security risks
    symbolic-links=0
    server-id=1
    log-bin=mysql-bin
    default-storage-engine=innodb
    character-set-client-handshake=FALSE
    character-set-server=utf8mb4
    collation-server=utf8mb4_unicode_ci
    init_connect='set character_set_database=utf8mb4'
    init_connect='set collation_database=utf8mb4_unicode_ci'
    port=34000

    [mysqldump]
    default-character-set=utf8mb4

    [mysqld_safe]
    log-error=/naver/appGuard/googlePlayParser/mysql_data/var/log/mysqld.log
    pid-file=/naver/appGuard/googlePlayParser/mysql_data/var/pid/mysqld.pid
  => 테스트용 4바이트 특수기호: https://vazor.com/unicode/c1F340.html
  => mysql db 셋업 가이드: https://mathiasbynens.be/notes/mysql-utf8mb4
  => 관련 설명: http://blog.lael.be/post/917
  => http://dev.mysql.com/doc/refman/5.7/en/charset-unicode.html
  => 참고, mysql 시스템 환경변수: http://dev.mysql.com/doc/refman/5.6/en/server-system-variables.html
 - show 커맨드
  => show variables like 'c%' : c로 시작하는 변수. 주로 collation, charset 전체를 보기 위해 사용
  => show variables like 'ver%' : mysql 버젼 체크용
  => show create table name : name 테이블이 어떻게 만들어졌는지 스키마 코드 출력. 매우 유용하다
 - table 속성 점검
  => desc tablename
  => show table status
 - 서버 종료
  => mysqladmin -u root --socket=/소켓경로지정필수/mysql.sock shutdown
 - 문자열 자료형 크기 정하기
  => 크기는 바이트 단위. 길이라고 헷갈리게 표현해도 바이트다.
  => utf8로 넣어도 ascii 표현 가능 문자는 1바이트
  => 기존 utf8은 3바이트
  => 특수기호 emoji 등은 4바이트
  => ex: "부산 지하철 도착 정보" -> length() -> 30
  => 이걸 계산해서 전체 사이즈를 결정해야 한다.
  => 자료형 정리: http://blog.munilive.com/mysql-%ED%95%84%EB%93%9C%ED%83%80%EC%9E%85%EC%9E%90%EB%A3%8C%ED%98%95-%EC%9A%94%EC%95%BD-%EC%A0%95%EB%A6%AC/
 - 데이터베이스 사이즈 알아내는 쿼리
  => 이거 고대로 복붙
    SELECT table_schema "Data Base Name", 
    sum( data_length + index_length ) / 1024 / 
    1024 "Data Base Size in MB", 
    sum( data_free )/ 1024 / 1024 "Free Space in MB" 
    FROM information_schema.TABLES 
    GROUP BY table_schema ; 
  => http://forums.mysql.com/read.php?108,201578,201578
 - MySQL 바이너리 로그
  => MySQL의 DB 파일들이 저장되어있는 database root directory(파라미터: --datadir) 안에는 DB 파일 외에 바이너리 로그가 mysql-bin.?????? 라는 이름으로 저장된다.
  => 아마도 replication, db 복원에도 사용하는 듯 하다 (최기린 팀장님 코멘트)
  => 이게 용량이 무시무시하므로 관리가 필요함
  => 로그 기록 활성화 옵션(mysqld): --log-bin
  => 로그 기록 활성화 옵션(my.cnf):
    [mysqld]
    log-bin=mysql-bin
  => mysql 안에서 로그 유지 날짜 설정: set global expire_logs_days=숫자; (0이 기본값인 듯. 아마 무제한?)
  => mysql 안에서 기록 여부 확인 및 관련 설정들:
    show variables like 'log_bin'; -> 바이너리 로그가 활성화되었는지 보여주는 값. read-only. root라도 db 안에서 수정 불가.
    show variables like 'expire_logs_days';
    show variables like 'binlog%';
    show variables like 'max_binlog%';
    show variables like 'log%';
  => 바이너리 로그를 읽는 프로그램: mysqlbinlog [로그 파일명]
  => mysql 안에서 파일 리스트 조회: show binary logs;
  => mysql 안에서 삭제
    purge binary logs to 'mysql-bin.000010' => 000001~000010 삭제
    purge binary logs before '2008-04-02 22:46:26' => 날짜 전 로그 삭제, 날짜 포맷은 mysql의 datetime, 'YYYY-MM-DD hh:mm:ss'
    binary와 master는 동의어 (ex: purge master logs)
  => 필요 권한(grant): SUPER, 혹은 REPLICATION CLIENT
  => https://dev.mysql.com/doc/refman/5.7/en/purge-binary-logs.html
  => http://blog.daum.net/moon0sool/83
  => http://blog.naver.com/tainow/120211674213
 - 커맨드 라인에 쿼리 입력 중 취소하고 싶을 때: 그냥 언제든 맨 끝에 "\c" 를 타이핑하고 엔터
 - 한 테이블의 내용을 다른 테이블에 insert
  => insert into table1 (col) select col from table2;

2. 리눅스 테크닉
 1) for + seq
  - for i in `seq 10`; do echo ${i}; done
  - http://www.snoopybox.co.kr/m/post/1680
 2) Command substitution
  - 명령줄 안에서 독립적인 명령문을 실행하는 기법
  - 셸 명령 실행 결과를 변수로 저장하거나 echo 커맨드를 이용해 출력한다는 것이 기본 의미
  - 방법
   i) grave accent (`): `echo "haha"`
   ii) 괄호 감싸기 ($()): $(echo "haha") -> "`" 는 중복시킬 수 없는데 이것은 중복이 가능하므로 다중 커맨드가 가능
    => ex) sort -m `for i in $(seq -w 11); do echo "file${i} "; done` | uniq > output
  - http://bash.cyberciti.biz/guide/Command_substitution
 3) sed, tr
  - 둘 다 text 치환 프로그램
  - sed: 단일 문자열 스트림 처리. 문자열, 정규식 특화
   => 기본 구분자는 "/". (ex: echo "test sentence" | sed s/abc/123/g)
   => 구분자 변경 가능. (ex: echo "test sentence" | sed 's-/home/greenfish/-/home2/greenfish/-' -> 구분자로 "-"가 사용되었다. 디렉토리 경로 처리할 때 유용)
   => -e: 스크립트 하나를 추가한다. 즉 여러 개의 명령을 처리하고 싶을 때 사용 가능 (ex: sed -e s/francois/FRANCOIS/g -e s/chris/CHRIS/g < myfile.txt)
  - tr: 문자열 처리. 단일 문자 특화
   => 기본: echo "I'm your father." | tr f F -> f를 F로 치환
   => -d: 문자 삭제 (ex: echo "1234 1234" | tr -d " ")
   => 범위지정: a-z, A-Z, a-Z, A-z, [:lower:], [:upper:] (ex: echo "asdf SDF SD2345" | tr a-z A-Z 혹은 tr [:lower:] [:upper:])
  - http://greenfishblog.tistory.com/66
 4) 문장 길이로 정렬하기
  - cat file | awk '{ print length, $0 }' | sort -n
  - http://stackoverflow.com/questions/5917576/sort-a-text-file-by-line-length-including-spaces
 5) 특정 줄만 선택하기
  - head + tail 조합은 파일이 크면 느릴 수도 있다. cat의 -n 옵션이 라인 번호를 붙여주는 점을 이용한다.
  - cat -n file | grep "^ *3" -> 3번째 줄을 선택한다.
  - awk 'NR==3' file
  - sed -n '5p' file
  - http://www.unix.com/unix-for-dummies-questions-and-answers/44651-see-specific-line-file.html
 6) nmap, netstat, lsof: 포트 확인하기 (새로운 서버 서비스를 시작할 때 비어있어서 사용할 수 있는 포트를 확인하기 위해)
  - nmap: 어떤 포트가 열려있는지 확인해준다.
   => nmap -sT -O localhost
  - netstat: 포트 정보를 보여준다.
   => netstat -anp
  - lsof: 찾아볼 것.
   => lsof -i
  - https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/3/html/Security_Guide/s1-server-ports.html

3. hadoop
 - 아파치 맵리듀스 튜토리얼 번역: http://cafe.naver.com/hadoopmania/32
 - 메이븐 리포지토리에 관련 패키지가 너무 많은데 뭘 써야 하나? -> org.apache.hadoop hadoop-client 쓰면 된다.
 - 하둡 버젼 1 -> 2로 올라가면서 바뀐 이름들 레퍼런스: https://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml
 - 과거 버젼 모음: http://archive.apache.org/dist/hadoop/core/
 - centOS 6.5 에서 하둡 깔고 제대로 64bit native library 올리기: 실패 ㅜㅜㅜ
  => 제작법 아파치 홈피 가이드: https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/NativeLibraries.html#Download
  => 블로거 가이드1: http://www.ercoppa.org/Linux-Compile-Hadoop-220-fix-Unable-to-load-native-hadoop-library.htm
  => 블로거 가이드2: http://bloodguy.tistory.com/entry/Hadoop-%EB%84%A4%EC%9D%B4%ED%8B%B0%EB%B8%8C-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC-%EB%B9%8C%EB%93%9C-build-native-library
  => protocol buffers(protoc): https://github.com/google/protobuf/releases?after=v2.6.0
  => gnu libtool: http://www.gnu.org/software/libtool/
 - Streaming MR
  - 자바 외에 다른 스크립트(루비, 파이썬, 리눅스 셸 등) 언어로 MR(MapReduce)을 짜서 돌릴 수 있다
  - 편리하다
  - 세밀한 제어에 한계가 있다
  - http://bloodguy.tistory.com/965
  - http://www.slideshare.net/TaeYoungLee1/20141111-mr-hadoopstreaming
  - 입력이 여러개면?
   => 폴더 이름을 지정 가능하다
  - 폴더가 여러개면?
   => -input 파라미터를 여러개 할 수 있다. ex) -input /dir1 -input /dir2 ...
  - 입력이 gzip 압축이면?
   => native로 지원한다. 그냥 넣는다
   => http://stackoverflow.com/questions/23230578/process-gzip-files-with-hadoop-streaming
  - 카운터 출력은?
   => stderr로 뽑은것이 카운터가 된다
  - https://hadoop.apache.org/docs/r1.2.1/streaming.html#How+do+I+specify+multiple+input+directories%3F
 - distcp
  - hdfs간 파일 복사
  - 실행 위치: destination hdfs
  - destination의 모든 cluster는 source의 모든 cluster host name을 등록해야 함 (/etc/hosts)
  - 명령어: hadoop distcp hdfs://source hdfs://destination
  - 다른 버젼의 하둡간 복사에는 hdfs 말고 hftp 프로토콜을 써야 한다.
   => hftp: HftpFileSystem. 읽기 전용, distcp 명령은 목적지 클러스터(태스크 트래커)에서 실행되어야 함. 포맷은 hftp://<dfs.http.address>/<path>.
   => <dfs.http.address>의 기본값은 <namenode>:50070
  - hadoop distcp (-option) source destination
   => ex) hadoop distcp -i hftp://10.24.170.22:50070/user/irteam/dmp/log/dmp_log/2015/11/ hdfs://tcshphdp-01b101:9000/user/irteam/DMP/
  - https://hadoop.apache.org/docs/r1.2.1/distcp.html
 - MultipleInputs
  => 서로 다른 위치에 있는 파일을 매퍼에 넣을 때 쓰는 클래스
  => http://www.lichun.cc/blog/2012/05/hadoop-multipleinputs-usage/
 - partitioner, comparator(grouping, sorting)
  => 파티셔너는 매퍼 결과를 키값을 기준으로 모아 리듀서로 보내주는 정책을 정하고 수행한다. 기본 파티셔너가 있어 키를 기준으로 해쉬값을 사용해 모아서 리듀서에 보내주는데, 직접 만든 하둡 데이터가 있고 키값이 다른 모양을 하고 있다면 이에 맞춰 파티셔너를 짜줘야 한다.
  => comparator는 새로운 사용자 정의 하둡 데이터 타입을 만들었을 때 데이터간 비교 방법을 재정의하는 것으로, 파티셔닝 이후 그룹핑/정렬을 할 때 필요하다.
  => http://huskdoll.tistory.com/175
 - mapreduce.inputformat.class org.apache.hadoop.mapreduce.lib.input.DelegatingInputFormat
  => oozie에서 특별히 이슈가 되는 부분이다.
  => 자바 MR은 SequenceFileInputFormat 안에 이 녀석이 이미 구현되어있어 일부러 의식하고 건드릴 필요가 없지만, oozie를 쓸 때는 이 녀석을 지정해줘야 한다.
 - Counter
  - MR 잡을 수행하는 중간 결과를 집계하여 잡트래커에서 확인할 수 있다.
  - 자바 MR은 enum으로 구현하고, context.getCounter(LogMapCounters.ITEMS8).increment(1); 와 같은 식으로 누적시켜 사용한다.
  - http://10.24.58.46:50030/jobdetailshistory.jsp?logFile=file:/nhnent/cslog/var/log/hadoop/history/done/version-1/tcshphdp-01b101_1446084138722_/2015/11/03/000000/job_201510291102_0891_1446512122737_irteam_oozie%253Aaction%253AT%253Dmap-reduce%253AW%253Dmerge%253AA%253Duniq-app-count
  - 하둡 잡 트래커가 카운터 데이터를 파싱해서 내놓는 api도 있다는데 아직 모르겠다. 조사 필요.
  - http://develop.sunshiny.co.kr/899
 - NullOutputFormat
  - 리듀서 출력을 안 내보낼 수 있다.
  - 그래도 output 폴더 설정은 해야 하고, MR잡 결과 파일같은 것이 남아있다
  - 카운터만 쓴다면 결과물이 필요없으므로 유용하다
  - org.apache.hadoop.mapred.lib.NullOutputFormat (1버젼대나 그 이하)
   => http://dcslab.snu.ac.kr/~hhyuck/hadoopdoc/api/index.html?org/apache/hadoop/mapred/lib/NullOutputFormat.html
  - org.apache.mapreduce.lib.NullOutputFormat (2버젼대)
   => https://hadoop.apache.org/docs/r2.4.1/api/index.html?org/apache/hadoop/mapreduce/lib/output/NullOutputFormat.html
  - 하둡 스트리밍에서는 파라미터와 함꼐 이것을 풀 클래스 패스로 넣어준다
   => hadoop jar /하둡경로/hadoop-streaming-2.0.0.jar -input 경로 -output 경로 -mapper 매퍼파일 -reducer 리듀서파일 -outputformat org.apache.hadoop.mapred.lib.NullOutputFormat

4. oozie
 - 하둡의 워크플로우 스케쥴러 시스템
 - 단위 작업을 나타내는 워크플로우(workflow)와 워크플로우를 자동으로 작동 및 반복시키는 코디네이터(coordinator)로 구성
 - 코디네이터는 시간 빈도와 데이터 가용 여부로 작동 가능
 - 데이터 가용 여부를 조건으로 걸었는데 데이터가 없을 때 기본 동작: 만들어질 때까지 큐에서 기다림
 - 워크플로우도, 코디네이터도 하나의 잡(job)으로 부름
 - xml로 작성
 - 변수, 함수 사용 가능. 글로벌 변수, 함수 존재
  => 사용 포맷: ${startTime}, ${punk}, ${global.date}
  => 정의 포맷: startTime=60, punk=beautiful
 - https://github.com/yahoo/oozie, http://oozie.apache.org/
 1) 워크플로우
  - 액션: 작업의 수행/연산(맵리듀스, pig, 셸 커맨드 등). ≒태스크, 액션노드
  - 워크플로우: 액션의 모음.
  - 워크플로우 소스코드 구성: workflow.xml + job.properties
  - 잡 프로퍼티(job properties, parameters)
    => 워크플로우 안에 사용된 잡 프로퍼티(변수, ${something})는 프로퍼티 파일에 반드시 명시되어야 하며 하나라도 빠지면 잡은 fail 상태에 빠진다.
    => 유효한 자바 식별자([A-Za-z_][0-9A-Za-z_]*)만으로 쓰여진 프로퍼티는 그대로 이름으로 쓸 수 있다. (ex: ${helloThere})
    => 그 외 문자가 쓰인 프로퍼티는 wf:conf(String name) 함수로 불러올 수 있다. (ex: wf:conf("job.tracker"))
    => job.properties에서 변수를 설정해서 xml에서 사용할 수 있다
  - 워크플로우 노드
   => 시작 제어 노드: 워크플로우 잡의 시작을 알리는 노드(액션)
   => 종료 제어 노드: 워크플로우의 성공적인 끝을 알리는 노드 (SUCCEEDED)
   => 킬 제어 노드: 워크플로우 잡에게 죽여달라고 알리는 노드. 즉 에러로 인한 종료 (KILLED)
   => 선택 제어 노드: 분기 노드. switch-case로 분기를 탄다
   => 포크 & 조인 제어 노드: 포크 노드는 동시에 여러 개 액션을 실행하고, 조인 노드는 이전의 포크 노드에서 실행된 모든 병렬 작업이 도착할 때까지 기다린다
  - 코드 설명 (workflow.xml)
    <workflow-app xmlns="uri:oozie:workflow:0.4" name="${appName}">
    => 워크플로우 시작.
      <global>
      => 글로벌 설정. 각 액션마다 공통적으로 들어가는 잡 트래커나 네임노드 설정을 여기서 잡는다
        <job-tracker>${jobTracker}</job-tracker>
        => 잡 트래커 이름 지정
        <name-node>${nameNode}</name-node>
        => 네임노드 이름 지정
        <configuration>
        => 하둡 잡에 필요한 각종 설정값이 name:value 페어로 들어감
          <property>
            <name>mapred.mapper.new-api</name>
            <value>true</value>
          </property>
        </configuration>
      </global>
      <start to="suffix-check"/>
      => 시작노드: 워크플로우 잡의 시작지점, 시작 액션을 알리는 노드
      <!-- comments -->
      => 코멘트 쓰는 법
      <action name="suffix-check">
      => 액션 정의
        <shell xmlns="uri:oozie:shell-action:0.2">
        => 셸 액션: 셸 커맨드를 실행시킨다. 워크플로우 잡은 셸 커맨드가 끝날때까지 기다림. 잡트래커, 네임노드, 셸 exec 엘리먼트 설정 등이 중요
        => http://10.24.58.46:11000/oozie/docs/DG_ShellActionExtension.html
        => 예제에 따르면 0.1을 쓰는데 우리 팀은 0.2 쓴다. 차이점?
          <job-tracker>${jobTracker}</job-tracker>
          <name-node>${nameNode}</name-node>
          <configuration>
            <property>
              <name>mapred.job.queue.name</name>
              <value>${queueName}</value>
            </property>
          </configuration>
          <exec>${scriptPath}</exec>
          => 실행할 셸 스크립트가 풀 패스로 들어감
          <argument>${date}</argument>
          => 셸 커맨드에 전달될 인자값
          <env-var>var1=value1</env-var>
          => 셸 커맨드에 전달될 환경변수값. 한 번에 하나의 페어만 전달 가능. 여러개 하려면 여러 엘리먼트 만들어라
          => 주의: 유닉스 변수를 사용하려면 유닉스 컨벤션을 써라(ex: $PATH). ${PATH} 하지 마라. oozie의 EL로 인식될 수 있다.
          <file>${HDFSscriptFullPath}#${scriptFilename}</file>
          => 정확히 모르겠다???
          <capture-output/>
          => 이걸 쓰면 oozie가 셸 커맨드 실행 결과로 나오는 STDOUT을 캡쳐한다. 출력물은 java properties 파일 포맷이어야 하고 2kb 초과할 수 없음
          => 워크플로우 정의에 의해, 결과물은 String action:output(String node, String key) 함수로 접근 가능
        </shell>
        <ok to="dist-copy"/>
        => 액션이 잘 끝날 경우 다음 경로
        <error to="fail"/>
        => 액션이 실패했을 경우 다음 경로
      </action>
      <action name="dist-copy" retry-max="50" retry-interval="5">
      => retry-max: 실패 시 50회까지 반복
      => retry-interval: 재시도 간격이 5초(추정)
        <distcp xmlns="uri:oozie:distcp-action:0.1">
        => distcp 액션: distcp를 수행하는 액션(추정)
          <job-tracker>${jobTracker}</job-tracker>
          <name-node>${nameNode}</name-node>
          <prepare>
          => 액션 수행 직전 사전 작업으로 결과물을 저장할 장소를 미리 지우고 새로 폴더를 만드는 과정을 등록
            <delete path="${nameNode}/user/${wf:user()}/${root}/output/${distcpDir}"/>
            <mkdir path="${nameNode}/user/${wf:user()}/${root}/output/${distcpDir}"/>
          </prepare>
          <configuration>
            <property>
              <name>distcpSuffix</name>
              <value>${wf:actionData('suffix-check')}['distcpSuffix']</value>
              => 조금 설명 필요?
            </property>
          </configuration>
          <arg>-i</arg>
          => distcp 액션에 아규먼트 전달. 실패 무시.
          <arg>${distcpSource}/${distcpPath}/${distYear}/${date}${distcpSuffix}</arg>
          => distcp src url
          <arg>${nameNode}/user/${wf:user()}/${root}/output/${distcpDir}/</arg>
          => distcp dest url
        </distcp>
        <ok to="decision-node"/>
        <error to="fail"/>
      </action>
      <decision name="decision-node">
        <switch>
          <case to="recent-merge">${date == "20150714"}</case>
          <default to="recent-multi-merge"/>
        </switch>
      </decision>
      <action name="recent-merge">
        <map-reduce>
          <prepare>
            <delete path="${nameNode}/user/${wf:user()}/${root}/output/${mergeDir}.${wf:id()}"/>
          </prepare>
          <configuration>
            <property>
              <name>mapreduce.map.class</name>
              <value>com.nhnent.search.hadoop.appguard.LogMapper</value>
            </property>
          </configuration>
        </map-reduce>
        <ok to="rename"/>
        <error to="fail"/>
      </action>
      <action name="recent-multi-merge">
        <map-reduce>
          <prepare>
            <delete path="${nameNode}/user/${wf:user()}/${root}/output/${mergeDir}.${wf:id()}"/>
          </prepare>
          <configuration>
            <property>
              <name>mapreduce.map.class</name>
              <value>org.apache.hadoop.mapreduce.lib.input.DelegatingMapper</value>
            </property>
          </configuration>
        </map-reduce>
        <ok to="rename"/>
        <error to="fail"/>
      </action>
      <action name="rename">
        <fs>
        => fs(hdfs) 액션. hdfs 파일, 디렉토리를 조작하게 해준다
          <delete path="/user/${wf:user()}/${root}/output/${mergeDir}"/>
          <move source="/user/${wf:user()}/${root}/output/${mergeDir}.${wf:id()}" target="/user/${wf:user()}/${root}/output/${mergeDir}"/>
        </fs>
        <ok to="forkjobs"/>
        <error to="fail"/>
      </action>
      <fork name="forkjobs">
      => 포크 액션. 동시에 여러 개 액션을 수행. 반드시 join 액션으로 닫아야 한다
        <path start="makecoll"/>
        <path start="uniq-app-count"/>
      </fork>
      <action name="uniq-app-count">
        <map-reduce>
          <prepare>
            <delete path="${nameNode}/user/${wf:user()}/${root}/output/${uniqAppDir}"/>
          </prepare>
          <configuration>
            <property>
              <name>mapreduce.map.class</name>
              <value>com.nhnent.search.hadoop.appguard.LogAppMapper</value>
            </property>
          </configuration>
        </map-reduce>
        <ok to="joining"/>
        <error to="fail"/>
      </action>
      <action name="makecoll">
        <map-reduce>
          <prepare>
            <delete path="${nameNode}/user/${wf:user()}/${root}/output/${collDir}"/>
          </prepare>
          <configuration>
            <property>
              <name>mapred.job.shuffle.input.buffer.percent</name>
              <value>0.20</value>
            </property>
          </configuration>
        </map-reduce>
        <ok to="joining"/>
        <error to="fail"/>
      </action>
      <join name="joining" to="end"/>
      <kill name="fail">
        <message>Map/Reduce failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
      </kill>
      <end name="end"/>
    </workflow-app>
 2) 코디네이터
  - 코디네이터 앱(app) = coordinator.xml(코디네이터 잡의 정의) + coordinator.properties(코디네이터 잡에 넘기고 싶은 설정값)
  - properties에서 변수를 설정해서 xml에서 사용할 수 있다
  - 코드 설명 (coordinator.xml)
    <coordinator-app name="MY_APP" frequency="60" start="2009-02-01T05:00Z" end="2009-02-01T06:00Z" timezone="UTC" xmlns="uri:oozie:coordinator:0.1">
    => "MY_APP" 이름으로 코디네이터 잡을 등록하고 시간은 60분, 09. 2. 1. 5시부터(이상) 6시까지(미만) 1시간동안 실행하므로 총 1회 실행. 타임존은 UTC. xmlns로 xml 문서 타입 지정
      <datasets>
      => 모든 입력 데이터셋의 메타데이터 정의
        <dataset name="input1" frequency="60" initial-instance="2009-01-01T00:00Z" timezone="UTC">
        => 데이터셋 설명: input1 이름으로 접근 가능한 데이터셋은 09.1.1 UTC시각으로 0시에 첫번째 데이터가 있으며 60분마다 데이터가 있음
          <uri-template>hdfs://localhost:9000/tmp/revenue_feed/${YEAR}/${MONTH}/${DAY}/${HOUR}</uri-template>
          => 데이터셋의 HDFS 디렉토리 구조. 여기까지만 지정하면 디렉토리 존재유무 검사가 됨. 데이터가 든 디렉토리가 없으면 대기.
          => /tmp/revenue_feed/2009/01/01/00/
          => /tmp/revenue_feed/2009/01/01/01/
          => /tmp/revenue_feed/2009/01/01/02/
          => ...
          <done-flag>trigger.dat</done-flag>
          => 파일명 지정. 여기까지 지정하면 파일 존재유무 검사가 됨. 데이터가 없으면 대기.
        </dataset>
      </datasets>
      <input-events>
      => 입력 파일 이벤트 설정
        <data-in name="coordInput1" dataset="input1">
        => dataset으로 input1을 바라보는 coordInput1 이름의 입력 데이터 정보를 작성
          <start-instance>${coord:current(-23)}</start-instance>
          => 시작 데이터는 데이터셋에서 23째로 오래된 데이터 -> coordinator-app 시작 시간이 09. 2. 1. 5시이므로 시작 데이터는 09. 1. 31. 6시
          <end-instance>${coord:current(0)}</end-instance>
          => 끝 데이터는 데이터셋 가장 최근 데이터 -> 시작데이터부터 지금 가장 최근 데이터 09. 2. 1. 0시까지.
        </data-in>
      </input-events>
      <action>
      => 동작 지정
        <workflow>
        => 워크플로우 지정
          <app-path>hdfs://localhost:9000/tmp/workflows</app-path>
          => 워크플로우 파일이 저장된 하둡 위치
          <configuration>
          => 워크플로우로 넘겨줄 설정 모음
            <property>
            => 워크플로우에 사용될 설정
              <name>input_files</name>
              => 이름은 input_files
              <value>${coord:dataIn('coordInput1')}</value>
              => <data-in>에서 설정했던 coordInput1 이름의 입력 데이터 hdfs 파일 경로 전부를 내보냄
            </property>
          </configuration>
        </workflow>
        <sla:info> ... <sla:info>
        => SLA compliance를 평가하기 위해 필요한 이벤트들을 기록하는 옵션이라는데 SLA가 뭔지 도무지 모르겠다. 해서 내부 조사 생략
        
      </action>
    </coordinator-app>
  - 코드 설명 (coordinator.properties)
   => 반드시 어느 코디네이터 xml을 가리키는지 파일 위치로 정의해야 한다. 이 한줄이 필요 -> oozie.coord.application.path=hdfs://localhost:9000/job/coord
   => 변수를 정의하고, xml에서 ${} 로 호출한다.
3) EL 변수, 함수
  - 글로벌 변수
    ${YEAR} : 년. YYYY
    ${MONTH} : 월. MM
    ${DAY} : 일. DD
    ${HOUR} : 시. HH
  - 코디네이터 함수
    ${coord:days(1)} : 하루
    ${coord:months(1)} : 한 달
    ${coord:current(0)} : 데이터셋에서 가장 최근 데이터를 리턴
    ${coord:current(-23)} : 데이터셋에서 23번째로 오래된 데이터를 리턴
    ${coord:dataIn('coordInput1')} : dataIn() 함수는 <data-in> 엘리먼트를 호출하는 함수로 보임. coordInput1 이름으로 엘리먼트를 찾음
    ${coord:nominalTime()} : 코디네이터 액션이 만들어진(materialization) datetime을 반환한다.
    ${coord:dateOffset(String baseDate, int instance, String timeUnit)} : EL 함수. 날짜를 방정식(newDate = baseDate + instance * timeUnit)으로 계산한다.
      => ${coord:dateOffset(coord:nominalTime(), -1, 'DAY')} : 현재 시간에서 하루 전날 값을 리턴
    ${coord:formatTime(String timestamp, String format)} : iso8601 표준 timestamp 문자열을 원하는 날짜 출력 format으로 변환한다
      => ${coord:formatTime(coord:dateOffset(coord:nominalTime(), -1, 'DAY'), 'yyyyMMdd')} : 현재 시간 하루 전날 값을 yyyyMMdd 형식으로 리턴
  - 기본 EL 상수: 전부 long 타입이다.
    KB: 1024, one kilobyte.
    MB: 1024 * KB, one megabyte.
    GB: 1024 * MB, one gigabyte.
    TB: 1024 * GB, one terabyte.
    PB: 1024 * TG, one petabyte.
  - 기본 EL 함수
    String firstNotNull(String value1, String value2): 첫번째 null 아닌 것을 반환, 둘다 null이면 null. null 리턴을 출력으로 사용하면 EL 라이브러리에 의해 빈 문자열로 인식됨.
    String concat(String s1, String s2): 2개 붙임. null은 빈 문자열 인식
    String replaceAll(String src, String regex, String replacement)
    String appendAll(String src, String append, String delimeter)
    String trim(String s)
    String urlEncode(String s)
    String timestamp()
    String toJsonStr(Map) (since Oozie 3.3)
    String toPropertiesStr(Map) (since Oozie 3.3)
    String toConfigurationStr(Map) (since Oozie 3.3)
    => http://10.24.58.46:11000/oozie/docs/WorkflowFunctionalSpec.html#a4.2.2_Basic_EL_Functions
  - 워크플로우 EL 함수
    String wf:id(): It returns the workflow job ID for the current workflow job.
    String wf:name(): It returns the workflow application name for the current workflow job.
    String wf:appPath(): It returns the workflow application path for the current workflow job.
    String wf:conf(String name): It returns the value of the workflow job configuration property for the current workflow job, or an empty string if undefined.
    String wf:user(): It returns the user name that started the current workflow job.
    String wf:group(): It returns the group/ACL for the current workflow job.
    String wf:callback(String stateVar): It returns the callback URL for the current workflow action node, stateVar can be a valid exit state (=OK= or ERROR ) for the action or a token to be replaced with the exit state by the remote system executing the task.
    String wf:transition(String node): It returns the transition taken by the specified workflow action node, or an empty string if the action has not being executed or it has not completed yet.
    String wf:lastErrorNode(): It returns the name of the last workflow action node that exit with an ERROR exit state, or an empty string if no a ction has exited with ERROR state in the current workflow job.
    String wf:errorCode(String node): It returns the error code for the specified action node, or an empty string if the action node has not exited with ERROR state. Each type of action node must define its complete error code list.
    String wf:errorMessage(String message): It returns the error message for the specified action node, or an empty string if no action node has not exited with ERROR state. The error message can be useful for debugging and notification purposes.
    int wf:run(): It returns the run number for the current workflow job, normally 0 unless the workflow job is re-run, in which case indicates the current run.
    Map wf:actionData(String node): This function is only applicable to action nodes that produce output data on completion. The output data is in a Java Properties format and via this EL function it is available as a Map .
    int wf:actionExternalId(String node): It returns the external Id for an action node, or an empty string if the action has not being executed or it has not completed yet.
    int wf:actionTrackerUri(String node): It returns the tracker URIfor an action node, or an empty string if the action has not being executed or it has not completed yet.
    int wf:actionExternalStatus(String node): It returns the external status for an action node, or an empty string if the action has not being executed or it has not completed yet.
    => http://10.24.58.46:11000/oozie/docs/WorkflowFunctionalSpec.html#a4.2.3_Workflow_EL_Functions
  - HDFS EL 함수
    boolean fs:exists(String path): It returns true or false depending if the specified path URI exists or not.
    boolean fs:isDir(String path): It returns true if the specified path URI exists and it is a directory, otherwise it returns false .
    boolean fs:dirSize(String path): It returns the size in bytes of all the files in the specified path. If the path is not a directory, or if it does not exist it returns -1. It does not work recursively, only computes the size of the files under the specified path.
    boolean fs:fileSize(String path): It returns the size in bytes of specified file. If the path is not a file, or if it does not exist it returns -1.
    boolean fs:blockSize(String path): It returns the block size in bytes of specified file. If the path is not a file, or if it does not exist it returns -1.
    => http://10.24.58.46:11000/oozie/docs/WorkflowFunctionalSpec.html#a4.2.7_HDFS_EL_Functions
4) 사용자 전달
    => 워크플로우 잡을 제출할 때 configuration 설정부에는 "user.name" 속성값이 반드시 사용되어야 한다. 보안 설정이 활성화되어있다면 oozie는 이 값이 웹 서비스의 프로토콜 리퀘스트에 담긴 사용자 증명서과 같은지 비교한다.
    => configuration에 oozie.job.acl(group.name이었는데 deprecated 되었다)이 사용될 수 있다. 권한 설정이 활성화되어있다면 이 값은 해당 잡의 ACL로 인식되고, 유저와 그룹 ID를 콤마 구분자로 가지고 있을 수 있다.
5) 주요 매핑
  - global에 쓰기 좋은 내용들
    mapred.mapper.new-api: 하둡 ver. 2 이후의 새 api명(대체로 mapred -> mapreduce)을 사용하기 위해 반드시 필요한 설정. true / false (boolean)
    mapred.reducer.new-api: 하둡 ver. 2 이후의 새 api명(대체로 mapred -> mapreduce)을 사용하기 위해 반드시 필요한 설정. true / false (boolean)
    mapred.job.queue.name(->mapreduce.job.queuename): 잡트래커에서 보이는 잡을 대기시키는 스케쥴링 큐의 이름. 기본값 default (string)
    mapred.map.tasks(->mapreduce.job.maps): 잡당 매퍼 개수. 기본값 2 (int)
    mapred.reduce.tasks(->mapreduce.job.reduces): 잡당 리듀서 개수. 보통 클러스터의 리듀서 처리 가용량의 99% 정도로 맞춘다. 노드가 fail될 때 reducer가 하나의 흐름을 유지하도록. 기본값 1 (int)
    mapred.compress.map.output(->mapreduce.map.output.compress): 결과물 압축 여부. 시퀀스파일 압축을 사용. 기본값 false (boolean)
    mapred.map.output.compression.codec(->mapreduce.map.output.compress.codec): 매퍼의 출력물 압축 코덱 지정. 기본값 org.apache.hadoop.io.compress.DefaultCodec (string)
    mapred.output.compress(->mapreduce.output.fileoutputformat.compress): 잡 출력의 압축 여부. 기본값 false (boolean)
    mapred.output.compression.type(->mapreduce.output.fileoutputformat.compress.type): 잡 출력이 시퀀스파일 포맷일 경우 압축 형식 지정. 기본값 RECORD (string) (권장: BLOCK, RECORD를 묶은 BLOCK 단위 압축. 더 빠른 것으로 알려져있다)
    mapred.output.compression.codec(->mapreduce.output.fileoutputformat.compress.codec): 잡 출력물 압축 코덱 지정. 기본값 org.apache.hadoop.io.compress.DefaultCodec (string)
  - 지역 잡 설정
    mapreduce.map.class(->mapreduce.job.map.class): 자바 소스의 매퍼 클래스명을 지정 (string)
    mapreduce.reduce.class(->mapreduce.job.reduce.class): 자바 소스의 리듀서 클래스명을 지정 (string)
    mapred.output.key.class(->mapreduce.job.output.key.class): 잡 출력물의 키값으로 사용할 클래스명 지정 (string)
    mapred.output.value.class(->mapreduce.job.output.value.class): 잡 출력물의 밸류값으로 사용할 클래스명 지정 (string)
    mapred.mapoutput.key.class(->mapreduce.map.output.key.class): 매퍼 출력물의 키값으로 사용할 클래스명 (string)
    mapred.mapoutput.value.class(->mapreduce.map.output.value.class): 매퍼 출력물의 밸류값으로 사용할 클래스명 (string)
    mapreduce.inputformat.class(->mapreduce.job.inputformat.class): 잡 입력물의 저장 포맷으로 사용할 클래스명 지정 (string)
      => org.apache.hadoop.mapreduce.lib.input.DelegatingInputFormat: 만약 MultipleInputs를 사용해서 입력을 받고자 한다면 이 설정을 반드시 해줘야 한다. 자바 코딩에서는 이 부분이 내부적으로 처리되어 있음.
    mapreduce.outputformat.class(->mapreduce.job.outputformat.class): 잡 출력물의 저장 포맷으로 사용할 클래스명 지정 (string)
    mapred.output.value.groupfn.class(->mapreduce.job.output.group.comparator.class): secondary sort에 사용할 grouping comparator 클래스 지정 (string)
    mapreduce.partitioner.class(->mapreduce.job.partitioner.class): 파티셔너 클래스 지정 (string)
    mapred.input.dir(->mapreduce.input.fileinputformat.inputdir): 입력 디렉토리 "가 아니라 파일" 설정. *.gz 같이 복수도 지정 가능 (string)
    mapred.output.dir(->mapreduce.output.fileoutputformat.outputdir): 출력 디렉토리 설정 (string)
    mapred.job.shuffle.input.buffer.percent(->mapreduce.reduce.shuffle.input.buffer.percent): The percentage of memory to be allocated from the maximum heap size to storing map outputs during the shuffle.
    mapred.input.dir.formats(->mapreduce.input.multipleinputs.dir.formats): 잡 입력 포맷이 MultipleInput일 경우 읽고자 하는 경로와 읽는 포맷 방식을 설정한다. 경로와 포맷 페어는 구분자 세미콜론, 페어간 구분자는 콤마
    mapred.input.dir.mappers(->mapreduce.input.multipleinputs.dir.mappers): 매퍼 입력 포맷이 MultipleInput일 경우 읽고자 하는 경로와 읽는 포맷 방식을 설정한다. 경로와 포맷 페어는 구분자 세미콜론, 페어간 구분자는 콤마

5. libcurl
 - `curl` 을 c 프로그래밍으로 사용할 수 있다.
 - 튜토리얼: http://www.joinc.co.kr/w/Site/Web/documents/UsedCurl
 - libcurl 에러코드표: http://linux.die.net/man/3/libcurl-errors
 
6. R
 - 무료 통계 그래프 그려주는 프로그램
 - 단순, 강력
 - https://www.r-project.org/
 - for문: http://www.r-bloggers.com/how-to-write-the-first-for-loop-in-r/
 - if문: https://stat.ethz.ch/pipermail/r-help/2005-September/079001.html
 - 그래프에 찍는 점의 사이즈 조절: http://stackoverflow.com/questions/2579995/control-the-size-of-points-in-an-r-scatterplot
 - as.hexmode(num), 숫자를 16진수로: https://stat.ethz.ch/R-manual/R-devel/library/base/html/hexmode.html
 - 자료형 - 데이터 프레임, 벡터
  - http://www.tutorialspoint.com/r/r_data_types.htm
  - http://www.r-tutor.com/r-introduction/data-frame/data-frame-column-vector
 - val <- c(), 벡터에 값 집어넣기: http://stackoverflow.com/questions/22235809/append-value-to-empty-vector-in-r
 - read.table("filename"), 외부 파일을 데이터 프레임으로 가져오기: http://www.r-tutor.com/r-introduction/data-frame/data-import
 - head, tail: http://rfunction.com/archives/699
 - factor(data), 사분범위 표시: http://127.0.0.1:23814/library/base/html/factor.html
 - 한글 튜토리얼
  - http://rstudio-pubs-static.s3.amazonaws.com/24275_7554c7b09c5c4a7fb85801316a6955cc.html
  - http://allaboutmoon.tistory.com/entry/R%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-%EA%B7%B8%EB%9E%98%ED%94%84-%EC%9E%91%EC%84%B1%ED%95%98%EA%B8%B0
  - http://blog.naver.com/kist125/90157253230